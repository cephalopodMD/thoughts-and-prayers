{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# knn, SVM, random forest\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas\n",
    "print(\"starting\")\n",
    "df = pandas.read_csv(\"hourly.csv\", sep=',', error_bad_lines=False) #ignore bad linesfrom random import random\n",
    "# 2600 tweets over 5000\n",
    "# 6600 tweets over 1000\n",
    "threshold = 10\n",
    "threshcount = len(df[df['Retweet'] > threshold])\n",
    "def keep(retweets):\n",
    "    if retweets > threshold:\n",
    "        return True\n",
    "    else:\n",
    "        if random() < float(threshcount) / float(60000-threshcount):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "df = df[df['Retweet'].map(keep)]\n",
    "print(df.columns)\n",
    "print(df.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = df[pandas.notnull(df['Text'])]\n",
    "print(df.columns)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(subset = 'Text')\n",
    "print(df.columns)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set threshold for binary classifier\n",
    "#retweets = df.Retweet\n",
    "# train_labels = []\n",
    "# for num in retweets:\n",
    "#     if num >= threshold:\n",
    "#         train_labels.append(1) #viral\n",
    "#     else:\n",
    "#         train_labels.append(0) #not viral\n",
    "df['Viral'] =df['Retweet'].apply(lambda retweet: 1 if retweet >= threshold else 0)\n",
    "train_labels = df.Viral.values\n",
    "labels = list(set(train_labels))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from feature_extractors5 import *\n",
    "# url_pattern = re.compile(r'(http(s?)://)[\\w./]+')\n",
    "# df['Text_URL'] =df['Text'].apply(lambda doc: url_pattern.sub(lambda x: domain_name_portable(x.group()), doc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "from nltk.stem.snowball import *\n",
    "stemmer = SnowballStemmer('english')\n",
    "from feature_extractors5 import *\n",
    "#stemmer = PorterStemmer('english')\n",
    "\n",
    "#stopwords?\n",
    "#stop = ['amp', 'cc', 'did', 'don', 'rt', 'll', 'oh', 've', 'yes', 'let', 'going', 'via', 're', 'tweet' ]\n",
    "stop = []\n",
    "#http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction\n",
    "# preProcess(str):\n",
    "#     url_pattern = re.compile(r'http(s?)://[\\w./]+')\n",
    "#     pic_pattern = re.compile(r'pic.twitter.com/[\\w.]+')\n",
    "#     str = pic_pattern.sub(\"\", str)\n",
    "#     str = url_pattern.sub(\"\", str)\n",
    "#     return str\n",
    "# http://shahmirj.com/blog/extracting-twitter-usertags-using-regex\n",
    "class NoUrls_CountVectorizer(CountVectorizer):\n",
    "    def build_preprocessor(self):\n",
    "        url_pattern = re.compile(r'http(s?)://[\\w./]+')\n",
    "        pic_pattern = re.compile(r'pic.twitter.com/[\\w.]+')\n",
    "        preprocessor = super(NoUrls_CountVectorizer, self).build_preprocessor()\n",
    "        return lambda doc: (pic_pattern.sub('', url_pattern.sub('', preprocessor(doc)) ))\n",
    "\n",
    "class NoUrls_Stemmed_CountVectorizer(CountVectorizer):\n",
    "    def build_preprocessor(self):\n",
    "        preprocessor = super(NoUrls_Stemmed_CountVectorizer, self).build_preprocessor()\n",
    "        url_pattern = re.compile(r'(http(s?)://)[\\w./]+')\n",
    "        at_pattern = re.compile(r' (?<=^|(?<=[^a-zA-Z0-9-_\\\\.]))@([A-Za-z]+[A-Za-z0-9_]+)')\n",
    "        return lambda doc: (at_pattern.sub('', url_pattern.sub(\"\", preprocessor(doc)) ))\n",
    "   #right now just doing splits on whitespace and stemming \n",
    "    def build_tokenizer(self):\n",
    "        tokenizer = super(NoUrls_Stemmed_CountVectorizer, self).build_tokenizer()\n",
    "        def process(word):\n",
    "            if (word.isdigit()):\n",
    "                if (int(word) >= 1800 and int(word) <= 2050):\n",
    "                    word = \"00000YEAR\"\n",
    "                else: \n",
    "                    word = \"00000NUM\"\n",
    "            else:\n",
    "                word =stemmer.stem(word)\n",
    "            return word\n",
    "        #return lambda doc: (stemmer.stem(w) for w in tokenizer(doc))\n",
    "        return lambda doc: (process(w) for w in tokenizer(doc))\n",
    "#bigrams? ngram_range=(1,2)\n",
    "vectorizer = NoUrls_Stemmed_CountVectorizer(ngram_range = (1,1), binary =True,\n",
    "                                    min_df=5, stop_words=stop, strip_accents='ascii')\n",
    "   \n",
    "#example processing\n",
    "# tweet = \"RT @femaIes: When you gotta go visit your broads in Atlanta https://t.co/pCau23tv3q\"\n",
    "# tweet = unicode(tweet, \"ascii\", \"ignore\")\n",
    "# print tweet\n",
    "# print 'Preprocess:', vectorizer.build_preprocessor()(tweet)\n",
    "# print\n",
    "# print 'Analyze:', vectorizer.build_analyzer()(tweet)\n",
    "# vectorizer.fit_transform([tweet])\n",
    "# vectorizer.vocabulary_  \n",
    "\n",
    "url_pattern = re.compile(r'(http(s?)://)[\\w./]+')\n",
    "print df.Text[1]\n",
    "print (url_pattern.sub(lambda x: domain_name_portable(x.group()), df.Text[1]))\n",
    "print \"done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "documents = df.Text\n",
    "X_count = vectorizer.fit_transform(documents)\n",
    "#print X_count\n",
    "#print X_count.toarray()\n",
    "print len(vectorizer.get_feature_names())\n",
    "print vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56131, 40744)\n",
      "(56131, 1)\n",
      "(56131, 40745)\n"
     ]
    }
   ],
   "source": [
    "#http://scikit-learn.org/stable/auto_examples/feature_stacker.html\n",
    "#trying to append to feature list X\n",
    "#http://stackoverflow.com/questions/19466868/how-do-i-do-classification-using-tfidfvectorizer-plus-metadata-in-practice\n",
    "import scipy\n",
    "linkCount = [df.LinkCount]\n",
    "linkCount = np.array(linkCount)\n",
    "X_metadata = scipy.sparse.csr_matrix(linkCount.T) #must transpose to get shape (___, 1)\n",
    "\n",
    "\n",
    "#print X_metadata\n",
    "#print X_count\n",
    "#X_metadata = [df.LinkCount.as_matrix()]\n",
    "print X_count.shape\n",
    "print X_metadata.shape\n",
    "X = scipy.sparse.hstack([X_count, X_metadata])\n",
    "#print X.toarray()\n",
    "\n",
    "print X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train labels: \n",
      "[0 0 0 ..., 0 0 0]\n",
      "[0, 1]\n",
      "\n",
      "train features:\n"
     ]
    }
   ],
   "source": [
    "train_labels = np.array(train_labels)\n",
    "train_features = X\n",
    "\n",
    "print \"train labels: \"\n",
    "print train_labels\n",
    "print labels\n",
    "print \n",
    "print \"train features:\"\n",
    "#print train_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Make SVM classifier\n",
    "from sklearn import svm\n",
    "#linear or not?\n",
    "name = \"Liblinear\"\n",
    "#loss=?, penalty=?, dual=False?, tol=1e-3\n",
    "classifier = svm.LinearSVC()\n",
    "classifier.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#http://stackoverflow.com/questions/25250654/how-can-i-use-a-custom-feature-selection-function-in-scikit-learns-pipeline\n",
    "#http://zacstewart.com/2014/08/05/pipelines-of-featureunions-of-pipelines.html\n",
    "\n",
    "from sklearn.base import TransformerMixin\n",
    "from pandas import DataFrame\n",
    "#define custom transformers\n",
    "class HourOfDayTransformer(TransformerMixin):\n",
    "    #gets hour from datetime field\n",
    "    def transform(self, X, **transform_params):\n",
    "        hours = X['DateTime'].apply(lambda x: int(x.split(\" \")[3].split(\":\")[0]))\n",
    "        return hours\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "import math\n",
    "class FollowersTransformer(TransformerMixin):\n",
    "    #gets hour from datetime field\n",
    "    def transform(self, X, **transform_params):\n",
    "        followers = X['Followers'].apply(lambda x: int(x) if isinstance(x, str) and x.isdigit() else 0)\n",
    "        return followers\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "class ColumnExtractor(TransformerMixin):\n",
    "    def __init__(self, colName):\n",
    "        self.colName = colName\n",
    "        \n",
    "    def transform(self, X, **transform_params):\n",
    "        text = X[self.colName].values\n",
    "        return text\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "class NormalizeShape(TransformerMixin):\n",
    "    #X is in format df[''].values\n",
    "    def transform(self, X, **transform_params):\n",
    "        normalized_array = scipy.sparse.csr_matrix(np.array((X))).T\n",
    "        return normalized_array\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from feature_extractors import *\n",
    "#define custom transformers using feature methods\n",
    "\n",
    "class HashTagCount(TransformerMixin):\n",
    "    #gets hashtag count\n",
    "    def transform(self, X, **transform_params):\n",
    "        hashcount = X['Text'].apply(lambda x: hashtag_count(x)) #custom function\n",
    "        return hashcount\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "class MentionCount(TransformerMixin):\n",
    "    def transform(self, X, **transform_params):\n",
    "        mentions = X['Text'].apply(lambda x: mention_count(x)) #custom function\n",
    "        return mentions\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "class CapitalCharRatio(TransformerMixin):\n",
    "    def transform(self, X, **transform_params):\n",
    "        ratios = X['Text'].apply(lambda x: capital_char_fraction(x)) #custom function\n",
    "        return ratios\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "class PhoneCount(TransformerMixin):\n",
    "    def transform(self, X, **transform_params):\n",
    "        counts = X['Text'].apply(lambda x: email_count(x)) #custom function\n",
    "        return counts\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "class EmailCount(TransformerMixin):\n",
    "    def transform(self, X, **transform_params):\n",
    "        counts = X['Text'].apply(lambda x: phone_count(x)) #custom function\n",
    "        return counts\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "class HappyCount(TransformerMixin):\n",
    "    def transform(self, X, **transform_params):\n",
    "        counts = X['Text'].apply(lambda x:  emoticons(x)[\"happy\"]) #custom function\n",
    "        return counts\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "class SadCount(TransformerMixin):\n",
    "    def transform(self, X, **transform_params):\n",
    "        counts = X['Text'].apply(lambda x:  emoticons(x)[\"sad\"]) #custom function\n",
    "        return counts\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "class PunctuationCount(TransformerMixin):\n",
    "    def __init__(self, punctuation):\n",
    "        self.punctuation = punctuation\n",
    "        \n",
    "    def transform(self, X, **transform_params):\n",
    "        counts = X['Text'].apply(lambda x:  alt_punctuation_counts(x)[self.punctuation]) #custom function\n",
    "        return counts\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "class TweetLength(TransformerMixin):\n",
    "    def transform(self, X, **transform_params):\n",
    "        counts = X['Text'].apply(lambda x:  tweet_length(x)) #custom function\n",
    "        return counts\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "class CamelCounts(TransformerMixin):\n",
    "    def transform(self, X, **transform_params):\n",
    "        counts = X['Text'].apply(lambda x:  camel_case_words(x)) #custom function\n",
    "        return counts\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "class UpperCounts(TransformerMixin):\n",
    "    def transform(self, X, **transform_params):\n",
    "        counts = X['Text'].apply(lambda x: all_caps_words(x)) #custom function\n",
    "        return counts\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "class LowerCounts(TransformerMixin):\n",
    "    def transform(self, X, **transform_params):\n",
    "        counts = X['Text'].apply(lambda x: lowercase_words(x)) #custom function\n",
    "        return counts\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "\n",
    "# hashCount = df['Text'].apply(lambda x: emoticons(x)[\"happy\"])\n",
    "# for x in range (200):\n",
    "#     print hashCount[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "(56131L,)\n",
      "(56131, 1)\n"
     ]
    }
   ],
   "source": [
    "#linkCount2 = scipy.sparse.csr_matrix(np.array((df['LinkCount'].values))).T\n",
    "#print linkCount2.shape\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dateTime = df['DateTime'].apply(lambda x: int(x.split(\" \")[3].split(\":\")[0]))\n",
    "normalized_array = scipy.sparse.csr_matrix(np.array((dateTime))).T\n",
    "print dateTime[0]\n",
    "print dateTime.shape\n",
    "print normalized_array.shape\n",
    "#print dateTime.split(\" \")[3].split(\":\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Time:', 253.9409999847412)\n",
      "('Total tweets classified:', 56131)\n",
      "('Score:', 0.53873217821848463)\n",
      "Confusion matrix:\n",
      "[[51325   729]\n",
      " [ 2291  1786]]\n",
      "SelectKBest(k=4000, score_func=<function chi2 at 0x000000000F232DD8>)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'FeatureUnion' object has no attribute 'columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-64-6355a049ed5b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnamed_steps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'feature_selector'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 122\u001b[1;33m \u001b[1;32mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    123\u001b[0m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'FeatureUnion' object has no attribute 'columns'"
     ]
    }
   ],
   "source": [
    "#Note score went down when including timeofday\n",
    "\n",
    "#Make a pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import time\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('binary_unigram', Pipeline([ \n",
    "            ('extract', ColumnExtractor(colName = 'Text')),\n",
    "            ('vectorizer',   NoUrls_CountVectorizer(ngram_range = (1,1), binary =True,\n",
    "                                            min_df=2, stop_words=stop, strip_accents='ascii'))\n",
    "        ])),\n",
    "         ('semicolon_count', Pipeline([\n",
    "            ('extract', PunctuationCount(punctuation = ';')),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "         ('colon_count', Pipeline([\n",
    "            ('extract', PunctuationCount(punctuation = ':')),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('questionmark_count', Pipeline([\n",
    "            ('extract', PunctuationCount(punctuation = '?')),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('exclamation_count', Pipeline([\n",
    "            ('extract', PunctuationCount(punctuation = '!')),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('hyphen_count', Pipeline([\n",
    "            ('extract', PunctuationCount(punctuation = '-')),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('period_count', Pipeline([\n",
    "            ('extract', PunctuationCount(punctuation = '.')),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "         ('quotation_count', Pipeline([\n",
    "            ('extract', PunctuationCount(punctuation = '\"')),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "         ('apostrophe_count', Pipeline([\n",
    "            ('extract', PunctuationCount(punctuation = '\\'')),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "         ('sad_count', Pipeline([\n",
    "            ('extract', SadCount()),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('happy_count', Pipeline([\n",
    "            ('extract', HappyCount()),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('phone_count', Pipeline([\n",
    "            ('extract', PhoneCount()),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('email_count', Pipeline([\n",
    "            ('extract', EmailCount()),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('hashtag_count', Pipeline([\n",
    "            ('extract', HashTagCount()),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('mention_count', Pipeline([\n",
    "            ('extract', MentionCount()),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('link_count', Pipeline([\n",
    "            ('extract', ColumnExtractor(colName = 'LinkCount')),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('follower_count', Pipeline([\n",
    "            ('extract', FollowersTransformer()),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('hourofday', Pipeline([\n",
    "            ('gethour', HourOfDayTransformer()),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "    ])),\n",
    "    ('feature_selector', SelectKBest(chi2, k=4000)),\n",
    "    ('classifier',  svm.LinearSVC(penalty='l1', dual=False, C=0.1)) \n",
    "])\n",
    "\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "#adapted from http://zacstewart.com/2015/04/28/document-classification-with-scikit-learn.html\n",
    "k_fold = KFold(n=len(df), n_folds=10)\n",
    "scores = []\n",
    "confusion = np.array([[0, 0], [0, 0]])\n",
    "\n",
    "for train_indices, test_indices in k_fold:\n",
    "    #train_text = df.iloc[train_indices]['Text'].values\n",
    "    train_text = df.iloc[train_indices]\n",
    "    train_y = df.iloc[train_indices]['Viral'].values\n",
    "    \n",
    "    #test_text = df.iloc[test_indices]['Text'].values\n",
    "    test_text = df.iloc[test_indices]\n",
    "    test_y = df.iloc[test_indices]['Viral'].values\n",
    "    \n",
    "    pipeline.fit(train_text, train_y)\n",
    "    predictions = pipeline.predict(test_text)\n",
    "\n",
    "    #update totals\n",
    "    confusion += confusion_matrix(test_y, predictions)\n",
    "    score = f1_score(test_y, predictions, pos_label=1)\n",
    "    scores.append(score)\n",
    "t1 = time.time()\n",
    "total = t1-t0\n",
    "print('Time:', total)\n",
    "print('Total tweets classified:', len(df))\n",
    "print('Score:', sum(scores)/len(scores))\n",
    "print('Confusion matrix:')\n",
    "print(confusion)\n",
    "\n",
    "X = pipeline.named_steps['features']\n",
    "features = pipeline.named_steps['feature_selector']\n",
    "print (features)\n",
    "print (features.transform(np.arange(len(X.columns))))\n",
    "print(X.columns[features.transform(np.arange(len(X.columns)))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Time:', 182.2849998474121)\n",
      "('Total tweets classified:', 56131)\n",
      "('Score:', 0.060369253454227688)\n",
      "Confusion matrix:\n",
      "[[54889    49]\n",
      " [ 1154    39]]\n"
     ]
    }
   ],
   "source": [
    "#Make kNN classifier\n",
    "#NearestCentroid, KNeighborsClassifier\n",
    "from sklearn import neighbors\n",
    "import time\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "name = \"kNN\"\n",
    "# classifier = neighbors.\n",
    "# KNeighborsClassifier(n_neighbors = 10)\n",
    "# classifier.fit(train_features, train_labels)\n",
    "\n",
    "#Make a pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer',   NoUrls_CountVectorizer(ngram_range = (1,1), binary =True,\n",
    "                                    min_df=1, stop_words=stop, strip_accents='ascii')),\n",
    "    ('classifier',  neighbors.KNeighborsClassifier(n_neighbors = 4)) ])\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('binary_unigram', Pipeline([ \n",
    "            ('extract', ColumnExtractor(colName = 'Text')),\n",
    "            ('vectorizer',   NoUrls_CountVectorizer(ngram_range = (1,1), binary =True,\n",
    "                                            min_df=2, stop_words=stop, strip_accents='ascii'))\n",
    "        ])),\n",
    "         ('semicolon_count', Pipeline([\n",
    "            ('extract', PunctuationCount(punctuation = ';')),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "         ('colon_count', Pipeline([\n",
    "            ('extract', PunctuationCount(punctuation = ':')),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('questionmark_count', Pipeline([\n",
    "            ('extract', PunctuationCount(punctuation = '?')),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('exclamation_count', Pipeline([\n",
    "            ('extract', PunctuationCount(punctuation = '!')),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('hyphen_count', Pipeline([\n",
    "            ('extract', PunctuationCount(punctuation = '-')),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('period_count', Pipeline([\n",
    "            ('extract', PunctuationCount(punctuation = '.')),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "         ('quotation_count', Pipeline([\n",
    "            ('extract', PunctuationCount(punctuation = '\"')),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "         ('apostrophe_count', Pipeline([\n",
    "            ('extract', PunctuationCount(punctuation = '\\'')),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "         ('sad_count', Pipeline([\n",
    "            ('extract', SadCount()),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('happy_count', Pipeline([\n",
    "            ('extract', HappyCount()),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('phone_count', Pipeline([\n",
    "            ('extract', PhoneCount()),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('email_count', Pipeline([\n",
    "            ('extract', EmailCount()),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('hashtag_count', Pipeline([\n",
    "            ('extract', HashTagCount()),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('mention_count', Pipeline([\n",
    "            ('extract', MentionCount()),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('link_count', Pipeline([\n",
    "            ('extract', ColumnExtractor(colName = 'LinkCount')),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('hourofday', Pipeline([\n",
    "            ('gethour', HourOfDayTransformer()),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "    ])),\n",
    "    ('classifier',  neighbors.KNeighborsClassifier(n_neighbors = 4))) \n",
    "])\n",
    "\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "#adapted from http://zacstewart.com/2015/04/28/document-classification-with-scikit-learn.html\n",
    "k_fold = KFold(n=len(df), n_folds=10)\n",
    "scores = []\n",
    "confusion = np.array([[0, 0], [0, 0]])\n",
    "\n",
    "for train_indices, test_indices in k_fold:\n",
    "    train_text = df.iloc[train_indices]['Text'].values\n",
    "    train_y = df.iloc[train_indices]['Viral'].values\n",
    "    \n",
    "    test_text = df.iloc[test_indices]['Text'].values\n",
    "    test_y = df.iloc[test_indices]['Viral'].values\n",
    "    \n",
    "    pipeline.fit(train_text, train_y)\n",
    "    predictions = pipeline.predict(test_text)\n",
    "\n",
    "    #update totals\n",
    "    confusion += confusion_matrix(test_y, predictions)\n",
    "    score = f1_score(test_y, predictions, pos_label=1)\n",
    "    scores.append(score)\n",
    "t1 = time.time()\n",
    "total = t1-t0\n",
    "print('Time:', total)\n",
    "print('Total tweets classified:', len(df))\n",
    "print('Score:', sum(scores)/len(scores))\n",
    "print('Confusion matrix:')\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Time:', 708.1180000305176)\n",
      "('Total tweets classified:', 59983)\n",
      "('Score:', 0.79366287749578812)\n",
      "Confusion matrix:\n",
      "[[57263    82]\n",
      " [  846  1792]]\n"
     ]
    }
   ],
   "source": [
    "#Make Random Forest clasisifier\n",
    "from sklearn import ensemble\n",
    "# name = \"randomforest\"\n",
    "# classifier = ensemble.RandomForestClassifier(n_estimators=100)\n",
    "# classifier.fit(train_features, train_labels)\n",
    "\n",
    "#Make a pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "import time\n",
    "\n",
    "t0 = time.time()\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer',   NoUrls_CountVectorizer(ngram_range = (1,1), binary =True,\n",
    "                                    min_df=1, stop_words=stop, strip_accents='ascii')),\n",
    "    ('classifier',  ensemble.RandomForestClassifier(n_estimators=10, max_features = 10)) ])\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('binary_unigram', Pipeline([ \n",
    "            ('extract', ColumnExtractor(colName = 'Text')),\n",
    "            ('vectorizer',   NoUrls_CountVectorizer(ngram_range = (1,1), binary =True,\n",
    "                                            min_df=2, stop_words=stop, strip_accents='ascii'))\n",
    "        ])),\n",
    "         ('semicolon_count', Pipeline([\n",
    "            ('extract', PunctuationCount(punctuation = ';')),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "         ('colon_count', Pipeline([\n",
    "            ('extract', PunctuationCount(punctuation = ':')),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('questionmark_count', Pipeline([\n",
    "            ('extract', PunctuationCount(punctuation = '?')),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('exclamation_count', Pipeline([\n",
    "            ('extract', PunctuationCount(punctuation = '!')),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('hyphen_count', Pipeline([\n",
    "            ('extract', PunctuationCount(punctuation = '-')),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('period_count', Pipeline([\n",
    "            ('extract', PunctuationCount(punctuation = '.')),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "         ('quotation_count', Pipeline([\n",
    "            ('extract', PunctuationCount(punctuation = '\"')),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "         ('apostrophe_count', Pipeline([\n",
    "            ('extract', PunctuationCount(punctuation = '\\'')),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "         ('sad_count', Pipeline([\n",
    "            ('extract', SadCount()),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('happy_count', Pipeline([\n",
    "            ('extract', HappyCount()),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('phone_count', Pipeline([\n",
    "            ('extract', PhoneCount()),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('email_count', Pipeline([\n",
    "            ('extract', EmailCount()),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('hashtag_count', Pipeline([\n",
    "            ('extract', HashTagCount()),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('mention_count', Pipeline([\n",
    "            ('extract', MentionCount()),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('link_count', Pipeline([\n",
    "            ('extract', ColumnExtractor(colName = 'LinkCount')),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('hourofday', Pipeline([\n",
    "            ('gethour', HourOfDayTransformer()),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "    ])),\n",
    "    ('classifier',  ensemble.RandomForestClassifier(n_estimators=10, max_features = 10))) \n",
    "])\n",
    "\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "#adapted from http://zacstewart.com/2015/04/28/document-classification-with-scikit-learn.html\n",
    "k_fold = KFold(n=len(df), n_folds=10)\n",
    "scores = []\n",
    "confusion = np.array([[0, 0], [0, 0]])\n",
    "\n",
    "for train_indices, test_indices in k_fold:\n",
    "    train_text = df.iloc[train_indices]['Text'].values\n",
    "    train_y = df.iloc[train_indices]['Viral'].values\n",
    "    \n",
    "    test_text = df.iloc[test_indices]['Text'].values\n",
    "    test_y = df.iloc[test_indices]['Viral'].values\n",
    "    \n",
    "    pipeline.fit(train_text, train_y)\n",
    "    predictions = pipeline.predict(test_text)\n",
    "\n",
    "    #update totals\n",
    "    confusion += confusion_matrix(test_y, predictions)\n",
    "    score = f1_score(test_y, predictions, pos_label=1)\n",
    "    scores.append(score)\n",
    "t1 = time.time()\n",
    "total = t1-t0\n",
    "print('Time:', total)\n",
    "print('Total tweets classified:', len(df))\n",
    "print('Score:', sum(scores)/len(scores))\n",
    "print('Confusion matrix:')\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Time:', 235.39699983596802)\n",
      "('Total tweets classified:', 56131)\n",
      "('Score:', 0.85985592153478607)\n",
      "Confusion matrix:\n",
      "[[27375  2847]\n",
      " [ 4208 21701]]\n"
     ]
    }
   ],
   "source": [
    "#Make naive_bayes\n",
    "from sklearn import naive_bayes\n",
    "# name = \"randomforest\"\n",
    "# classifier = ensemble.RandomForestClassifier(n_estimators=100)\n",
    "# classifier.fit(train_features, train_labels)\n",
    "\n",
    "#Make a pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "import time\n",
    "\n",
    "t0 = time.time()\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer',   NoUrls_CountVectorizer(ngram_range = (1,1), binary =True,\n",
    "                                    min_df=1, stop_words=stop, strip_accents='ascii')),\n",
    "    ('classifier',  naive_bayes.BernoulliNB()) ])\n",
    "\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('binary_unigram', Pipeline([ \n",
    "            ('extract', ColumnExtractor(colName = 'Text')),\n",
    "            ('vectorizer',   NoUrls_CountVectorizer(ngram_range = (1,1), binary =True,\n",
    "                                            min_df=2, stop_words=stop, strip_accents='ascii'))\n",
    "        ])),\n",
    "         ('semicolon_count', Pipeline([\n",
    "            ('extract', PunctuationCount(punctuation = ';')),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "         ('colon_count', Pipeline([\n",
    "            ('extract', PunctuationCount(punctuation = ':')),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('questionmark_count', Pipeline([\n",
    "            ('extract', PunctuationCount(punctuation = '?')),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('exclamation_count', Pipeline([\n",
    "            ('extract', PunctuationCount(punctuation = '!')),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('hyphen_count', Pipeline([\n",
    "            ('extract', PunctuationCount(punctuation = '-')),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('period_count', Pipeline([\n",
    "            ('extract', PunctuationCount(punctuation = '.')),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "         ('quotation_count', Pipeline([\n",
    "            ('extract', PunctuationCount(punctuation = '\"')),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "         ('apostrophe_count', Pipeline([\n",
    "            ('extract', PunctuationCount(punctuation = '\\'')),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "         ('sad_count', Pipeline([\n",
    "            ('extract', SadCount()),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('happy_count', Pipeline([\n",
    "            ('extract', HappyCount()),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('phone_count', Pipeline([\n",
    "            ('extract', PhoneCount()),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('email_count', Pipeline([\n",
    "            ('extract', EmailCount()),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('hashtag_count', Pipeline([\n",
    "            ('extract', HashTagCount()),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('mention_count', Pipeline([\n",
    "            ('extract', MentionCount()),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('link_count', Pipeline([\n",
    "            ('extract', ColumnExtractor(colName = 'LinkCount')),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('hourofday', Pipeline([\n",
    "            ('gethour', HourOfDayTransformer()),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "    ])),\n",
    "    ('classifier',  naive_bayes.BernoulliNB()) \n",
    "])\n",
    "\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "#adapted from http://zacstewart.com/2015/04/28/document-classification-with-scikit-learn.html\n",
    "k_fold = KFold(n=len(df), n_folds=10)\n",
    "scores = []\n",
    "confusion = np.array([[0, 0], [0, 0]])\n",
    "\n",
    "for train_indices, test_indices in k_fold:\n",
    "    #train_text = df.iloc[train_indices]['Text'].values\n",
    "    train_text = df.iloc[train_indices]\n",
    "    train_y = df.iloc[train_indices]['Viral'].values\n",
    "    \n",
    "    #test_text = df.iloc[test_indices]['Text'].values\n",
    "    test_text = df.iloc[test_indices]\n",
    "    test_y = df.iloc[test_indices]['Viral'].values\n",
    "    \n",
    "    pipeline.fit(train_text, train_y)\n",
    "    predictions = pipeline.predict(test_text)\n",
    "\n",
    "    #update totals\n",
    "    confusion += confusion_matrix(test_y, predictions)\n",
    "    score = f1_score(test_y, predictions, pos_label=1)\n",
    "    scores.append(score)\n",
    "t1 = time.time()\n",
    "total = t1-t0\n",
    "print('Time:', total)\n",
    "print('Total tweets classified:', len(df))\n",
    "print('Score:', sum(scores)/len(scores))\n",
    "print('Confusion matrix:')\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('binary_unigram', Pipeline([ \n",
    "            ('extract', ColumnExtractor(colName = 'Text')),\n",
    "            ('vectorizer',   NoUrls_CountVectorizer(ngram_range = (1,1), binary =True,\n",
    "                                            min_df=2, stop_words=stop, strip_accents='ascii'))\n",
    "        ])),\n",
    "         ('semicolon_count', Pipeline([\n",
    "            ('extract', PunctuationCount(punctuation = ';')),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "         ('colon_count', Pipeline([\n",
    "            ('extract', PunctuationCount(punctuation = ':')),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('questionmark_count', Pipeline([\n",
    "            ('extract', PunctuationCount(punctuation = '?')),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('exclamation_count', Pipeline([\n",
    "            ('extract', PunctuationCount(punctuation = '!')),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('hyphen_count', Pipeline([\n",
    "            ('extract', PunctuationCount(punctuation = '-')),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('period_count', Pipeline([\n",
    "            ('extract', PunctuationCount(punctuation = '.')),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "         ('quotation_count', Pipeline([\n",
    "            ('extract', PunctuationCount(punctuation = '\"')),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "         ('apostrophe_count', Pipeline([\n",
    "            ('extract', PunctuationCount(punctuation = '\\'')),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "         ('sad_count', Pipeline([\n",
    "            ('extract', SadCount()),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('happy_count', Pipeline([\n",
    "            ('extract', HappyCount()),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('phone_count', Pipeline([\n",
    "            ('extract', PhoneCount()),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('email_count', Pipeline([\n",
    "            ('extract', EmailCount()),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('hashtag_count', Pipeline([\n",
    "            ('extract', HashTagCount()),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('mention_count', Pipeline([\n",
    "            ('extract', MentionCount()),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('link_count', Pipeline([\n",
    "            ('extract', ColumnExtractor(colName = 'LinkCount')),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('hourofday', Pipeline([\n",
    "            ('gethour', HourOfDayTransformer()),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "    ]))\n",
    "])\n",
    "X = pipeline.fit(df, df[\"Viral\"])\n",
    "print X.shape\n",
    "print X.columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chi2score = chi2(train_fea, df[\"Viral\"])[0]\n",
    "\n",
    "from pylab import barh,plot,yticks,show,grid,xlabel,figure\n",
    "figure(figsize=(10,20))\n",
    "wscores = zip(vectorizer.get_feature_names(), chi2score)\n",
    "wchi2 = sorted(wscores, key=lambda x:x[1])\n",
    "k=100\n",
    "topchi2 = zip(*wchi2[-k:])\n",
    "x = range(len(topchi2[1]))\n",
    "labels = topchi2[0]\n",
    "barh(x,topchi2[1],align='center',alpha=.2,color='g')\n",
    "plot(topchi2[1],x,'-o',markersize=2,alpha=.8,color='g')\n",
    "yticks(x,labels)\n",
    "xlabel('$\\chi^2$')\n",
    "show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
