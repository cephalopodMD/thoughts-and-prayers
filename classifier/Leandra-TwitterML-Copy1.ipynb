{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping line 418110: expected 18 fields, saw 22\n",
      "\n",
      "Skipping line 470334: expected 18 fields, saw 19\n",
      "\n",
      "/usr/lib/python2.7/dist-packages/pandas/io/parsers.py:1150: DtypeWarning: Columns (1,3,4,6,8,9,10,11,12,15,16) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = self._reader.read(nrows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([u'URL', u'Keywords', u'Keyword Count', u'DateTime', u'Favorite Count', u'Retweet', u'Lang', u'LinkCount', u'Link1', u'Link2', u'Link3', u'Author', u'Text', u'Followers', u'Friends', u'Location', u'Timezone', u'UTC Offset'], dtype='object')\n",
      "(566337, 18)\n"
     ]
    }
   ],
   "source": [
    "# knn, SVM, random forest\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas\n",
    "print(\"starting\")\n",
    "df = pandas.read_csv(\"corpus.csv\", sep='\\t', error_bad_lines=False) #ignore bad lines\n",
    "print(df.columns)\n",
    "print(df.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([u'URL', u'Keywords', u'Keyword Count', u'DateTime', u'Favorite Count', u'Retweet', u'Lang', u'LinkCount', u'Link1', u'Link2', u'Link3', u'Author', u'Text', u'Followers', u'Friends', u'Location', u'Timezone', u'UTC Offset'], dtype='object')\n",
      "(414540, 18)\n"
     ]
    }
   ],
   "source": [
    "df = df[pandas.notnull(df['Text'])]\n",
    "print(df.columns)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([u'URL', u'Keywords', u'Keyword Count', u'DateTime', u'Favorite Count', u'Retweet', u'Lang', u'LinkCount', u'Link1', u'Link2', u'Link3', u'Author', u'Text', u'Followers', u'Friends', u'Location', u'Timezone', u'UTC Offset'], dtype='object')\n",
      "(110093, 18)\n"
     ]
    }
   ],
   "source": [
    "df = df.drop_duplicates(subset = 'Text')\n",
    "print(df.columns)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2520\n",
      "(4896, 18)\n"
     ]
    }
   ],
   "source": [
    "# 2600 tweets over 5000\n",
    "# 6600 tweets over 1000\n",
    "threshold = 5000\n",
    "threshcount = len(df[df['Retweet'] > threshold])\n",
    "from random import random\n",
    "def keep(retweets):\n",
    "    if retweets > threshold:\n",
    "        return True\n",
    "    else:\n",
    "        if random() < float(threshcount) / float(len(df)-threshcount):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "df = df[df['Retweet'].map(keep)]\n",
    "print(threshcount)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set threshold for binary classifier\n",
    "#retweets = df.Retweet\n",
    "# train_labels = []\n",
    "# for num in retweets:\n",
    "#     if num >= threshold:\n",
    "#         train_labels.append(1) #viral\n",
    "#     else:\n",
    "#         train_labels.append(0) #not viral\n",
    "df['Viral'] =df['Retweet'].apply(lambda retweet: 1 if retweet >= threshold else 0)\n",
    "train_labels = df.Viral.values\n",
    "labels = list(set(train_labels))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from feature_extractors5 import *\n",
    "# url_pattern = re.compile(r'(http(s?)://)[\\w./]+')\n",
    "# df['Text_URL'] =df['Text'].apply(lambda doc: url_pattern.sub(lambda x: domain_name_portable(x.group()), doc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "from nltk.stem.snowball import *\n",
    "stemmer = SnowballStemmer('english')\n",
    "from features import *\n",
    "#stemmer = PorterStemmer('english')\n",
    "\n",
    "#stopwords?\n",
    "#stop = ['amp', 'cc', 'did', 'don', 'rt', 'll', 'oh', 've', 'yes', 'let', 'going', 'via', 're', 'tweet' ]\n",
    "stop = []\n",
    "#http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction\n",
    "# preProcess(str):\n",
    "#     url_pattern = re.compile(r'http(s?)://[\\w./]+')\n",
    "#     pic_pattern = re.compile(r'pic.twitter.com/[\\w.]+')\n",
    "#     str = pic_pattern.sub(\"\", str)\n",
    "#     str = url_pattern.sub(\"\", str)\n",
    "#     return str\n",
    "# http://shahmirj.com/blog/extracting-twitter-usertags-using-regex\n",
    "class NoUrls_CountVectorizer(CountVectorizer):\n",
    "    def build_preprocessor(self):\n",
    "        url_pattern = re.compile(r'http(s?)://[\\w./]+')\n",
    "        pic_pattern = re.compile(r'pic.twitter.com/[\\w.]+')\n",
    "        preprocessor = super(NoUrls_CountVectorizer, self).build_preprocessor()\n",
    "        return lambda doc: (pic_pattern.sub('', url_pattern.sub('', preprocessor(doc)) ))\n",
    "\n",
    "class NoUrls_Stemmed_CountVectorizer(CountVectorizer):\n",
    "    def build_preprocessor(self):\n",
    "        preprocessor = super(NoUrls_Stemmed_CountVectorizer, self).build_preprocessor()\n",
    "        url_pattern = re.compile(r'(http(s?)://)[\\w./]+')\n",
    "        at_pattern = re.compile(r' (?<=^|(?<=[^a-zA-Z0-9-_\\\\.]))@([A-Za-z]+[A-Za-z0-9_]+)')\n",
    "        return lambda doc: (at_pattern.sub('', url_pattern.sub(\"\", preprocessor(doc)) ))\n",
    "   #right now just doing splits on whitespace and stemming \n",
    "    def build_tokenizer(self):\n",
    "        tokenizer = super(NoUrls_Stemmed_CountVectorizer, self).build_tokenizer()\n",
    "        def process(word):\n",
    "            if (word.isdigit()):\n",
    "                if (int(word) >= 1800 and int(word) <= 2050):\n",
    "                    word = \"00000YEAR\"\n",
    "                else: \n",
    "                    word = \"00000NUM\"\n",
    "            else:\n",
    "                word =stemmer.stem(word)\n",
    "            return word\n",
    "        #return lambda doc: (stemmer.stem(w) for w in tokenizer(doc))\n",
    "        return lambda doc: (process(w) for w in tokenizer(doc))\n",
    "#bigrams? ngram_range=(1,2)\n",
    "vectorizer = NoUrls_Stemmed_CountVectorizer(ngram_range = (1,1), binary =True,\n",
    "                                    min_df=5, stop_words=stop, strip_accents='ascii')\n",
    "   \n",
    "#example processing\n",
    "# tweet = \"RT @femaIes: When you gotta go visit your broads in Atlanta https://t.co/pCau23tv3q\"\n",
    "# tweet = unicode(tweet, \"ascii\", \"ignore\")\n",
    "# print tweet\n",
    "# print 'Preprocess:', vectorizer.build_preprocessor()(tweet)\n",
    "# print\n",
    "# print 'Analyze:', vectorizer.build_analyzer()(tweet)\n",
    "# vectorizer.fit_transform([tweet])\n",
    "# vectorizer.vocabulary_  \n",
    "\n",
    "url_pattern = re.compile(r'(http(s?)://)[\\w./]+')\n",
    "print \"done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1308\n",
      "['00000NUM', '00000YEAR', u'00pm', u'0mm', u'0mph', u'10pm', u'1st', u'2c', u'2nd', u'30secondflght', u'3rd', u'4am', u'5sos', u'6c', u'9gag', u'___', u'abl', u'about', u'absolut', u'account', u'achiev', u'act', u'actual', u'ad', u'add', u'admit', u'ador', u'af', u'afeni', u'after', u'again', u'against', u'age', u'ago', u'agre', u'ain', u'air', u'airport', u'al', u'album', u'aliv', u'all', u'allah', u'alon', u'along', u'alreadi', u'also', u'alway', u'am', u'amaz', u'america', u'american', u'amp', u'an', u'and', u'angel', u'angri', u'ani', u'anim', u'anniversari', u'annoy', u'anoth', u'answer', u'anti', u'anymor', u'anyon', u'anyth', u'anyway', u'ap', u'app', u'appear', u'appl', u'appreci', u'approach', u'april', u'are', u'area', u'aren', u'argu', u'arm', u'around', u'arrest', u'arriv', u'art', u'articl', u'artist', u'as', u'ask', u'ass', u'at', u'ate', u'attach', u'attack', u'attent', u'attitud', u'author', u'avail', u'award', u'away', u'awesom', u'babe', u'babi', u'back', u'bad', u'bae', u'ball', u'ban', u'bank', u'bar', u'baromet', u'basic', u'bath', u'bathroom', u'battl', u'bc', u'be', u'beach', u'beat', u'beauti', u'becaus', u'becom', u'bed', u'bee', u'been', u'befor', u'begin', u'behind', u'believ', u'bell', u'best', u'bet', u'better', u'between', u'beyonc', u'bibl', u'bid', u'big', u'biggest', u'bill', u'birdman', u'birth', u'birthday', u'bit', u'bitch', u'black', u'bless', u'blog', u'blue', u'bodi', u'boo', u'book', u'born', u'both', u'bought', u'bout', u'box', u'boy', u'boyfriend', u'brazil', u'break', u'breakfast', u'breath', u'bring', u'bro', u'broke', u'brother', u'brought', u'brown', u'bruh', u'bts', u'bubbl', u'build', u'bump', u'busi', u'but', u'buy', u'by', u'cake', u'california', u'call', u'came', u'camera', u'campaign', u'can', u'candid', u'cannot', u'cant', u'car', u'carat', u'card', u'care', u'carpet', u'case', u'cat', u'catch', u'caus', u'celebr', u'challeng', u'champion', u'chanc', u'chang', u'channel', u'cheat', u'check', u'chees', u'chelsea', u'chicken', u'child', u'childhood', u'chill', u'china', u'chocol', u'choic', u'choke', u'choos', u'chris', u'citi', u'civil', u'clair', u'class', u'classic', u'clean', u'clear', u'climat', u'clinton', u'close', u'cloth', u'cloud', u'club', u'coachella', u'code', u'coffe', u'cold', u'collect', u'colleg', u'color', u'come', u'comfort', u'comment', u'common', u'compani', u'complet', u'compound', u'concept', u'confirm', u'confus', u'congrat', u'congratul', u'consid', u'constant', u'contact', u'continu', u'control', u'convers', u'convinc', u'cook', u'cool', u'copi', u'cost', u'could', u'couldn', u'countri', u'cover', u'crazi', u'creat', u'creativ', u'cri', u'cross', u'crowd', u'crush', u'cruz', u'cultur', u'cup', u'current', u'custom', u'cut', u'cute', u'cutest', u'cuti', u'cuz', u'dad', u'daddi', u'daili', u'damn', u'danc', u'dane', u'daniel', u'danni', u'dark', u'data', u'date', u'daughter', u'dawg', u'day', u'dead', u'deal', u'death', u'debut', u'decent', u'decid', u'decis', u'deep', u'definit', u'deserv', u'design', u'despit', u'destroy', u'detail', u'dick', u'did', u'didn', u'didnt', u'die', u'differ', u'dinner', u'direct', u'discuss', u'disney', u'dm', u'do', u'doctor', u'doe', u'doesn', u'doesnt', u'dog', u'don', u'donald', u'donat', u'done', u'dont', u'door', u'dope', u'doubl', u'down', u'drag', u'drake', u'draw', u'dream', u'dress', u'drink', u'drive', u'drop', u'drove', u'dude', u'dure', u'each', u'earli', u'earth', u'earthday', u'easili', u'eat', u'edit', u'effect', u'effort', u'either', u'eleanor', u'els', u'embarrass', u'emot', u'end', u'endoplasm', u'ene', u'energi', u'english', u'enjoy', u'enough', u'enter', u'entir', u'entri', u'epic', u'episod', u'even', u'event', u'ever', u'everi', u'everybodi', u'everyday', u'everyon', u'everyth', u'ex', u'exact', u'exam', u'excit', u'exclus', u'exist', u'expect', u'expert', u'explain', u'extra', u'eye', u'face', u'facebook', u'fact', u'fail', u'fair', u'fall', u'fals', u'famili', u'fan', u'far', u'fashion', u'fast', u'fat', u'father', u'fav', u'favorit', u'fear', u'featur', u'feed', u'feel', u'fell', u'felt', u'femal', u'few', u'field', u'fight', u'figur', u'fill', u'film', u'final', u'find', u'fine', u'finish', u'fire', u'first', u'first_album', u'fish', u'fit', u'five', u'fix', u'fli', u'flight', u'floor', u'flower', u'focus', u'follow', u'food', u'footbal', u'for', u'forev', u'forget', u'former', u'forward', u'found', u'free', u'freez', u'french', u'fresh', u'fri', u'friday', u'friend', u'from', u'front', u'ft', u'fuck', u'full', u'fun', u'funni', u'funniest', u'futur', u'gala', u'game', u'garden', u'gave', u'gay', u'genius', u'georg', u'get', u'gift', u'girl', u'girlfriend', u'give', u'giveaway', u'given', u'go', u'goal', u'god', u'goe', u'gold', u'gone', u'gonna', u'good', u'goodnight', u'gorgeous', u'gossip', u'got', u'gotta', u'grab', u'grade', u'graduat', u'grandma', u'graphic', u'great', u'greatest', u'green', u'grey', u'grill', u'group', u'grow', u'gt', u'guess', u'guid', u'gust', u'guy', u'had', u'haha', u'hahaha', u'hair', u'hand', u'hang', u'happen', u'happi', u'hard', u'harmoni', u'harri', u'harriet', u'has', u'hate', u'have', u'haven', u'havingaparti', u'he', u'head', u'health', u'hear', u'heard', u'heart', u'heaven', u'hell', u'hello', u'help', u'her', u'here', u'hero', u'hey', u'hi', u'high', u'hillari', u'him', u'his', u'histori', u'hit', u'hoe', u'hold', u'holi', u'home', u'homi', u'honest', u'honor', u'hop', u'hope', u'hors', u'host', u'hot', u'hotel', u'hour', u'hous', u'how', u'hpa', u'ht', u'htt', u'http', u'https', u'hug', u'huge', u'hum', u'human', u'humid', u'hurt', u'husband', u'icon', u'idc', u'idea', u'idk', u'if', u'ignor', u'ill', u'im', u'imag', u'imagin', u'import', u'improv', u'in', u'includ', u'increas', u'incred', u'indiana', u'insid', u'inspir', u'instagram', u'instead', u'interest', u'intern', u'internet', u'interview', u'into', u'invit', u'iphon', u'is', u'isn', u'isnt', u'issu', u'it', u'ive', u'jack', u'jackson', u'jail', u'jam', u'jame', u'jami', u'jesus', u'job', u'john', u'join', u'josh', u'judg', u'jump', u'june', u'just', u'justin', u'kany', u'keep', u'kendal', u'key', u'kick', u'kid', u'kill', u'kim', u'kind', u'kinda', u'king', u'kiss', u'km', u'knew', u'know', u'la', u'ladi', u'languag', u'larg', u'last', u'late', u'later', u'latest', u'laugh', u'launch', u'law', u'lay', u'lcfc', u'leagu', u'lean', u'learn', u'least', u'leav', u'lebron', u'lee', u'left', u'legend', u'leicest', u'less', u'lesson', u'let', u'letter', u'level', u'lie', u'life', u'light', u'like', u'lil', u'limit', u'lip', u'list', u'listen', u'lit', u'liter', u'littl', u'live', u'll', u'lmao', u'lmfao', u'lol', u'london', u'long', u'look', u'lord', u'lose', u'lost', u'lot', u'love', u'low', u'lt', u'luck', u'lucki', u'lunch', u'mad', u'made', u'magic', u'make', u'makeup', u'mall', u'mama', u'man', u'manag', u'manchest', u'mani', u'mark', u'market', u'marri', u'match', u'matter', u'may', u'mayb', u'mb', u'me', u'mean', u'meant', u'media', u'meet', u'member', u'meme', u'memori', u'men', u'mess', u'messag', u'met', u'metgala', u'mexican', u'mia', u'michael', u'midnight', u'might', u'mile', u'million', u'mind', u'mine', u'minut', u'miss', u'mission', u'mistak', u'mix', u'mm', u'model', u'mom', u'moment', u'monday', u'money', u'month', u'mood', u'more', u'morn', u'most', u'mother', u'move', u'movi', u'mph', u'mr', u'much', u'mum', u'music', u'must', u'my', u'myself', u'name', u'nation', u'natur', u'ne', u'near', u'need', u'negat', u'never', u'new', u'news', u'next', u'nice', u'nigga', u'night', u'no', u'nobodi', u'nomin', u'none', u'norman', u'north', u'not', u'note', u'noth', u'notic', u'now', u'number', u'nw', u'obama', u'obsess', u'of', u'off', u'offer', u'offici', u'oh', u'ok', u'okay', u'old', u'omg', u'on', u'onc', u'one', u'onli', u'onlin', u'open', u'opportun', u'or', u'order', u'origin', u'other', u'our', u'out', u'outsid', u'over', u'own', u'owner', u'pack', u'page', u'paid', u'pant', u'panther', u'paper', u'parent', u'pari', u'park', u'part', u'parti', u'pass', u'past', u'pat', u'paul', u'pay', u'peac', u'peopl', u'perfect', u'perform', u'period', u'person', u'phone', u'photo', u'physic', u'pic', u'pick', u'pictur', u'pink', u'piss', u'pizza', u'place', u'plan', u'planet', u'plant', u'play', u'player', u'pleas', u'pls', u'pm', u'pocket', u'point', u'polic', u'pool', u'poor', u'pop', u'porn', u'pose', u'posit', u'post', u'power', u'ppl', u'practic', u'pray', u'prayer', u'pre', u'premier', u'presid', u'pressur', u'pretti', u'princ', u'princess', u'print', u'prioriti', u'probabl', u'problem', u'process', u'product', u'program', u'prologu', u'prom', u'promis', u'promot', u'protect', u'proud', u'public', u'pull', u'puppi', u'purpl', u'put', u'queen', u'question', u'quick', u'quit', u'quot', u'race', u'racist', u'radio', u'rain', u'ralli', u'random', u'rate', u'rather', u'rdma', u're', u'reach', u'reaction', u'read', u'readi', u'real', u'realiz', u'realli', u'reason', u'receiv', u'recip', u'record', u'red', u'relat', u'relationship', u'releas', u'rememb', u'remind', u'remov', u'repli', u'report', u'respect', u'respons', u'rest', u'result', u'reticulum', u'retweet', u'reveal', u'review', u'ribosom', u'rich', u'ride', u'right', u'rihanna', u'ring', u'rip', u'ripami', u'ripprinc', u'rise', u'risk', u'river', u'rn', u'road', u'rock', u'roll', u'room', u'rough', u'round', u'rt', u'rts', u'ruin', u'rule', u'run', u'ryan', u'sad', u'safe', u'said', u'salad', u'sale', u'same', u'san', u'saturday', u'savag', u'save', u'saw', u'say', u'schedul', u'school', u'score', u'scream', u'season', u'second', u'secret', u'see', u'seem', u'seen', u'select', u'selfi', u'sell', u'semest', u'send', u'senior', u'sens', u'sent', u'seri', u'serious', u'serv', u'servic', u'set', u'settl', u'seventeen', u'sex', u'sexi', u'sexual', u'shade', u'shakur', u'share', u'she', u'shit', u'shock', u'shoe', u'shoot', u'shop', u'shot', u'should', u'shouldn', u'show', u'sick', u'side', u'sign', u'simpl', u'simpli', u'sinc', u'sing', u'singl', u'sister', u'sit', u'situat', u'size', u'skin', u'slay', u'sleep', u'slowli', u'small', u'smart', u'smell', u'smile', u'smoke', u'snap', u'snapchat', u'so', u'social', u'some', u'somebodi', u'someon', u'someth', u'sometim', u'son', u'song', u'soon', u'sorri', u'soul', u'sound', u'space', u'speak', u'special', u'speed', u'spend', u'spirit', u'sponsor', u'sport', u'spread', u'spur', u'squad', u'st', u'stage', u'stand', u'star', u'stare', u'start', u'state', u'statement', u'station', u'stay', u'steadi', u'step', u'still', u'stock', u'stole', u'stop', u'store', u'stori', u'straight', u'stream', u'street', u'stress', u'strong', u'stronger', u'struggl', u'student', u'studi', u'studio', u'stuff', u'stupid', u'style', u'success', u'such', u'sudden', u'summer', u'sun', u'super', u'suppli', u'support', u'suppos', u'sure', u'surpris', u'swear', u'sweet', u'system', u'take', u'taken', u'talent', u'talk', u'target', u'tea', u'teacher', u'team', u'tear', u'tech', u'technolog', u'tell', u'temp', u'temperatur', u'ten', u'test', u'text', u'tf', u'th', u'than', u'thank', u'that', u'the', u'their', u'them', u'theme', u'then', u'there', u'these', u'they', u'thing', u'think', u'this', u'those', u'though', u'thought', u'thousand', u'three', u'through', u'throw', u'throwback', u'ticket', u'till', u'time', u'timelin', u'tire', u'titl', u'to', u'today', u'togeth', u'told', u'tomorrow', u'tonight', u'too', u'took', u'tool', u'top', u'touch', u'tour', u'trade', u'traffic', u'train', u'tran', u'travel', u'treat', u'tree', u'tri', u'tribut', u'trick', u'trip', u'true', u'truli', u'trump', u'trump2016', u'trust', u'truth', u'tryna', u'tubman', u'tuesday', u'tune', u'tupac', u'turn', u'tv', u'tweet', u'twice', u'twitter', u'two', u'type', u'ugli', u'uk', u'understand', u'unit', u'univers', u'until', u'up', u'updat', u'ur', u'us', u'usa', u'use', u'user', u'vardi', u've', u'veri', u'version', u'via', u'vibe', u'vid', u'video', u'view', u'vine', u'visit', u'voic', u'vote', u'vs', u'wait', u'wake', u'waldorf', u'walk', u'wall', u'wanna', u'want', u'war', u'was', u'wasn', u'wast', u'watch', u'water', u'way', u'we', u'wear', u'weather', u'web', u'wed', u'week', u'weekend', u'weird', u'welcom', u'well', u'went', u'were', u'west', u'what', u'when', u'where', u'whi', u'which', u'while', u'white', u'who', u'whole', u'wife', u'wild', u'will', u'win', u'wind', u'window', u'winner', u'wish', u'with', u'without', u'woke', u'woman', u'women', u'won', u'wonder', u'wont', u'word', u'work', u'world', u'worri', u'wors', u'worth', u'would', u'wouldn', u'wow', u'write', u'written', u'wrong', u'wrote', u'wtf', u'xoxo', u'xx', u'ya', u'yall', u'yeah', u'year', u'yes', u'yesterday', u'yet', u'yo', u'york', u'you', u'young', u'youngforev', u'your', u'yourself', u'youtub', u'zayn']\n"
     ]
    }
   ],
   "source": [
    "documents = df.Text\n",
    "X_count = vectorizer.fit_transform(documents)\n",
    "#print X_count\n",
    "#print X_count.toarray()\n",
    "print len(vectorizer.get_feature_names())\n",
    "print vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4896, 1308)\n",
      "(4896, 1)\n",
      "(4896, 1309)\n"
     ]
    }
   ],
   "source": [
    "#http://scikit-learn.org/stable/auto_examples/feature_stacker.html\n",
    "#trying to append to feature list X\n",
    "#http://stackoverflow.com/questions/19466868/how-do-i-do-classification-using-tfidfvectorizer-plus-metadata-in-practice\n",
    "import scipy\n",
    "linkCount = [df.LinkCount]\n",
    "linkCount = np.array(linkCount)\n",
    "X_metadata = scipy.sparse.csr_matrix(linkCount.T) #must transpose to get shape (___, 1)\n",
    "\n",
    "\n",
    "#print X_metadata\n",
    "#print X_count\n",
    "#X_metadata = [df.LinkCount.as_matrix()]\n",
    "print X_count.shape\n",
    "print X_metadata.shape\n",
    "X = scipy.sparse.hstack([X_count, X_metadata])\n",
    "#print X.toarray()\n",
    "\n",
    "print X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train labels: \n",
      "[1 0 1 ..., 1 0 0]\n",
      "[0, 1]\n",
      "\n",
      "train features:\n"
     ]
    }
   ],
   "source": [
    "train_labels = np.array(train_labels)\n",
    "train_features = X\n",
    "\n",
    "print \"train labels: \"\n",
    "print train_labels\n",
    "print labels\n",
    "print \n",
    "print \"train features:\"\n",
    "#print train_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python2.7/dist-packages/sklearn/svm/base.py:8: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility\n",
      "  from . import libsvm, liblinear\n",
      "/usr/lib/python2.7/dist-packages/sklearn/svm/base.py:9: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility\n",
      "  from . import libsvm_sparse\n",
      "/usr/lib/python2.7/dist-packages/scipy/interpolate/interpolate.py:28: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility\n",
      "  from . import _ppoly\n",
      "/usr/lib/python2.7/dist-packages/scipy/spatial/__init__.py:90: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility\n",
      "  from .ckdtree import *\n",
      "/usr/lib/python2.7/dist-packages/scipy/spatial/__init__.py:91: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility\n",
      "  from .qhull import *\n",
      "/usr/lib/python2.7/dist-packages/sklearn/linear_model/least_angle.py:24: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility\n",
      "  from ..utils import array2d, arrayfuncs, as_float_array, check_arrays\n",
      "/usr/lib/python2.7/dist-packages/sklearn/metrics/cluster/supervised.py:18: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility\n",
      "  from .expected_mutual_info_fast import expected_mutual_information\n",
      "/usr/lib/python2.7/dist-packages/sklearn/metrics/pairwise.py:56: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility\n",
      "  from .pairwise_fast import _chi2_kernel_fast, _sparse_manhattan\n",
      "/usr/lib/python2.7/dist-packages/sklearn/linear_model/coordinate_descent.py:26: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility\n",
      "  from . import cd_fast\n",
      "/usr/lib/python2.7/dist-packages/sklearn/linear_model/__init__.py:21: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility\n",
      "  from .sgd_fast import Hinge, Log, ModifiedHuber, SquaredLoss, Huber\n",
      "/usr/lib/python2.7/dist-packages/sklearn/utils/random.py:9: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility\n",
      "  from ._random import sample_without_replacement\n",
      "/usr/local/lib/python2.7/dist-packages/ipython-4.1.1-py2.7.egg/IPython/core/formatters.py:92: DeprecationWarning: DisplayFormatter._ipython_display_formatter_default is deprecated: use @default decorator instead.\n",
      "  def _ipython_display_formatter_default(self):\n",
      "/usr/local/lib/python2.7/dist-packages/ipython-4.1.1-py2.7.egg/IPython/core/formatters.py:669: DeprecationWarning: PlainTextFormatter._singleton_printers_default is deprecated: use @default decorator instead.\n",
      "  def _singleton_printers_default(self):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='l2', multi_class='ovr', penalty='l2',\n",
       "     random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Make SVM classifier\n",
    "from sklearn import svm\n",
    "#linear or not?\n",
    "name = \"Liblinear\"\n",
    "#loss=?, penalty=?, dual=False?, tol=1e-3\n",
    "classifier = svm.LinearSVC()\n",
    "classifier.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#http://stackoverflow.com/questions/25250654/how-can-i-use-a-custom-feature-selection-function-in-scikit-learns-pipeline\n",
    "#http://zacstewart.com/2014/08/05/pipelines-of-featureunions-of-pipelines.html\n",
    "\n",
    "from sklearn.base import TransformerMixin\n",
    "from pandas import DataFrame\n",
    "#define custom transformers\n",
    "class HourOfDayTransformer(TransformerMixin):\n",
    "    #gets hour from datetime field\n",
    "    def transform(self, X, **transform_params):\n",
    "        hours = X['DateTime'].apply(lambda x: int(x.split(\" \")[3].split(\":\")[0]))\n",
    "        return hours\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "import math\n",
    "class FollowersTransformer(TransformerMixin):\n",
    "    #gets hour from datetime field\n",
    "    def transform(self, X, **transform_params):\n",
    "        followers = X['Followers'].apply(lambda x: int(x) if isinstance(x, str) and x.isdigit() else 0)\n",
    "        return followers\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "class ColumnExtractor(TransformerMixin):\n",
    "    def __init__(self, colName):\n",
    "        self.colName = colName\n",
    "        \n",
    "    def transform(self, X, **transform_params):\n",
    "        text = X[self.colName].values\n",
    "        return text\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "class NormalizeShape(TransformerMixin):\n",
    "    #X is in format df[''].values\n",
    "    def transform(self, X, **transform_params):\n",
    "        normalized_array = scipy.sparse.csr_matrix(np.array((X))).T\n",
    "        return normalized_array\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from features import *\n",
    "#define custom transformers using feature methods\n",
    "\n",
    "class HashTagCount(TransformerMixin):\n",
    "    #gets hashtag count\n",
    "    def transform(self, X, **transform_params):\n",
    "        hashcount = X['Text'].apply(lambda x: hashtag_count(x)) #custom function\n",
    "        return hashcount\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "class MentionCount(TransformerMixin):\n",
    "    def transform(self, X, **transform_params):\n",
    "        mentions = X['Text'].apply(lambda x: mention_count(x)) #custom function\n",
    "        return mentions\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "class CapitalCharRatio(TransformerMixin):\n",
    "    def transform(self, X, **transform_params):\n",
    "        ratios = X['Text'].apply(lambda x: capital_char_fraction(x)) #custom function\n",
    "        return ratios\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "class PhoneCount(TransformerMixin):\n",
    "    def transform(self, X, **transform_params):\n",
    "        counts = X['Text'].apply(lambda x: email_count(x)) #custom function\n",
    "        return counts\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "class EmailCount(TransformerMixin):\n",
    "    def transform(self, X, **transform_params):\n",
    "        counts = X['Text'].apply(lambda x: phone_count(x)) #custom function\n",
    "        return counts\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "class HappyCount(TransformerMixin):\n",
    "    def transform(self, X, **transform_params):\n",
    "        counts = X['Text'].apply(lambda x:  emoticons(x)[\"happy\"]) #custom function\n",
    "        return counts\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "class SadCount(TransformerMixin):\n",
    "    def transform(self, X, **transform_params):\n",
    "        counts = X['Text'].apply(lambda x:  emoticons(x)[\"sad\"]) #custom function\n",
    "        return counts\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "class PunctuationCount(TransformerMixin):\n",
    "    def __init__(self, punctuation):\n",
    "        self.punctuation = punctuation\n",
    "        \n",
    "    def transform(self, X, **transform_params):\n",
    "        counts = X['Text'].apply(lambda x:  alt_punctuation_counts(x)[self.punctuation]) #custom function\n",
    "        return counts\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "class TweetLength(TransformerMixin):\n",
    "    def transform(self, X, **transform_params):\n",
    "        counts = X['Text'].apply(lambda x:  tweet_length(x)) #custom function\n",
    "        return counts\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "class CamelCounts(TransformerMixin):\n",
    "    def transform(self, X, **transform_params):\n",
    "        counts = X['Text'].apply(lambda x:  camel_case_words(x)) #custom function\n",
    "        return counts\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "class UpperCounts(TransformerMixin):\n",
    "    def transform(self, X, **transform_params):\n",
    "        counts = X['Text'].apply(lambda x: all_caps_words(x)) #custom function\n",
    "        return counts\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "class LowerCounts(TransformerMixin):\n",
    "    def transform(self, X, **transform_params):\n",
    "        counts = X['Text'].apply(lambda x: lowercase_words(x)) #custom function\n",
    "        return counts\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "\n",
    "# hashCount = df['Text'].apply(lambda x: emoticons(x)[\"happy\"])\n",
    "# for x in range (200):\n",
    "#     print hashCount[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#linkCount2 = scipy.sparse.csr_matrix(np.array((df['LinkCount'].values))).T\n",
    "#print linkCount2.shape\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dateTime = df['DateTime'].apply(lambda x: int(x.split(\" \")[3].split(\":\")[0]))\n",
    "normalized_array = scipy.sparse.csr_matrix(np.array((dateTime))).T\n",
    "#print dateTime.split(\" \")[3].split(\":\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Time:', 15.79653000831604)\n",
      "('Total tweets classified:', 4896)\n",
      "('Score:', 0.84345513701544106)\n",
      "Confusion matrix:\n",
      "[[1583  793]\n",
      " [  88 2432]]\n"
     ]
    }
   ],
   "source": [
    "#Note score went down when including timeofday\n",
    "\n",
    "#Make a pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import time\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('binary_unigram', Pipeline([ \n",
    "            ('extract', ColumnExtractor(colName = 'Text')),\n",
    "            ('vectorizer',   NoUrls_CountVectorizer(ngram_range = (1,1), binary =True,\n",
    "                                            min_df=2, stop_words=stop, strip_accents='ascii'))\n",
    "        ])),\n",
    "         ('semicolon_count', Pipeline([\n",
    "            ('extract', PunctuationCount(punctuation = ';')),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "         ('colon_count', Pipeline([\n",
    "            ('extract', PunctuationCount(punctuation = ':')),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('questionmark_count', Pipeline([\n",
    "            ('extract', PunctuationCount(punctuation = '?')),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('exclamation_count', Pipeline([\n",
    "            ('extract', PunctuationCount(punctuation = '!')),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('hyphen_count', Pipeline([\n",
    "            ('extract', PunctuationCount(punctuation = '-')),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('period_count', Pipeline([\n",
    "            ('extract', PunctuationCount(punctuation = '.')),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "         ('quotation_count', Pipeline([\n",
    "            ('extract', PunctuationCount(punctuation = '\"')),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "         ('apostrophe_count', Pipeline([\n",
    "            ('extract', PunctuationCount(punctuation = '\\'')),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "         ('sad_count', Pipeline([\n",
    "            ('extract', SadCount()),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('happy_count', Pipeline([\n",
    "            ('extract', HappyCount()),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('phone_count', Pipeline([\n",
    "            ('extract', PhoneCount()),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('email_count', Pipeline([\n",
    "            ('extract', EmailCount()),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('hashtag_count', Pipeline([\n",
    "            ('extract', HashTagCount()),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('mention_count', Pipeline([\n",
    "            ('extract', MentionCount()),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('link_count', Pipeline([\n",
    "            ('extract', ColumnExtractor(colName = 'LinkCount')),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('follower_count', Pipeline([\n",
    "            ('extract', FollowersTransformer()),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('hourofday', Pipeline([\n",
    "            ('gethour', HourOfDayTransformer()),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "    ])),\n",
    "    ('feature_selector', SelectKBest(chi2, k=1000)),\n",
    "    ('classifier',  svm.LinearSVC(penalty='l1', dual=False, C=0.1)) \n",
    "])\n",
    "\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "#adapted from http://zacstewart.com/2015/04/28/document-classification-with-scikit-learn.html\n",
    "k_fold = KFold(n=len(df), n_folds=10)\n",
    "scores = []\n",
    "confusion = np.array([[0, 0], [0, 0]])\n",
    "\n",
    "for train_indices, test_indices in k_fold:\n",
    "    #train_text = df.iloc[train_indices]['Text'].values\n",
    "    train_text = df.iloc[train_indices]\n",
    "    train_y = df.iloc[train_indices]['Viral'].values\n",
    "    \n",
    "    #test_text = df.iloc[test_indices]['Text'].values\n",
    "    test_text = df.iloc[test_indices]\n",
    "    test_y = df.iloc[test_indices]['Viral'].values\n",
    "    \n",
    "    pipeline.fit(train_text, train_y)\n",
    "    predictions = pipeline.predict(test_text)\n",
    "\n",
    "    #update totals\n",
    "    confusion += confusion_matrix(test_y, predictions)\n",
    "    score = f1_score(test_y, predictions, pos_label=1)\n",
    "    scores.append(score)\n",
    "t1 = time.time()\n",
    "total = t1-t0\n",
    "print('Time:', total)\n",
    "print('Total tweets classified:', len(df))\n",
    "print('Score:', sum(scores)/len(scores))\n",
    "print('Confusion matrix:')\n",
    "print(confusion)\n",
    "\n",
    "X = pipeline.named_steps['features']\n",
    "features = pipeline.named_steps['feature_selector']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Time:', 182.2849998474121)\n",
      "('Total tweets classified:', 56131)\n",
      "('Score:', 0.060369253454227688)\n",
      "Confusion matrix:\n",
      "[[54889    49]\n",
      " [ 1154    39]]\n"
     ]
    }
   ],
   "source": [
    "#Make kNN classifier\n",
    "#NearestCentroid, KNeighborsClassifier\n",
    "from sklearn import neighbors\n",
    "import time\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "name = \"kNN\"\n",
    "# classifier = neighbors.\n",
    "# KNeighborsClassifier(n_neighbors = 10)\n",
    "# classifier.fit(train_features, train_labels)\n",
    "\n",
    "#Make a pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer',   NoUrls_CountVectorizer(ngram_range = (1,1), binary =True,\n",
    "                                    min_df=1, stop_words=stop, strip_accents='ascii')),\n",
    "    ('classifier',  neighbors.KNeighborsClassifier(n_neighbors = 4)) ])\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('binary_unigram', Pipeline([ \n",
    "            ('extract', ColumnExtractor(colName = 'Text')),\n",
    "            ('vectorizer',   NoUrls_CountVectorizer(ngram_range = (1,1), binary =True,\n",
    "                                            min_df=2, stop_words=stop, strip_accents='ascii'))\n",
    "        ])),\n",
    "         ('semicolon_count', Pipeline([\n",
    "            ('extract', PunctuationCount(punctuation = ';')),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "         ('colon_count', Pipeline([\n",
    "            ('extract', PunctuationCount(punctuation = ':')),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('questionmark_count', Pipeline([\n",
    "            ('extract', PunctuationCount(punctuation = '?')),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('exclamation_count', Pipeline([\n",
    "            ('extract', PunctuationCount(punctuation = '!')),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('hyphen_count', Pipeline([\n",
    "            ('extract', PunctuationCount(punctuation = '-')),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('period_count', Pipeline([\n",
    "            ('extract', PunctuationCount(punctuation = '.')),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "         ('quotation_count', Pipeline([\n",
    "            ('extract', PunctuationCount(punctuation = '\"')),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "         ('apostrophe_count', Pipeline([\n",
    "            ('extract', PunctuationCount(punctuation = '\\'')),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "         ('sad_count', Pipeline([\n",
    "            ('extract', SadCount()),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('happy_count', Pipeline([\n",
    "            ('extract', HappyCount()),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('phone_count', Pipeline([\n",
    "            ('extract', PhoneCount()),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('email_count', Pipeline([\n",
    "            ('extract', EmailCount()),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('hashtag_count', Pipeline([\n",
    "            ('extract', HashTagCount()),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('mention_count', Pipeline([\n",
    "            ('extract', MentionCount()),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('link_count', Pipeline([\n",
    "            ('extract', ColumnExtractor(colName = 'LinkCount')),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('hourofday', Pipeline([\n",
    "            ('gethour', HourOfDayTransformer()),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "    ])),\n",
    "    ('classifier',  neighbors.KNeighborsClassifier(n_neighbors = 4))) \n",
    "])\n",
    "\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "#adapted from http://zacstewart.com/2015/04/28/document-classification-with-scikit-learn.html\n",
    "k_fold = KFold(n=len(df), n_folds=10)\n",
    "scores = []\n",
    "confusion = np.array([[0, 0], [0, 0]])\n",
    "\n",
    "for train_indices, test_indices in k_fold:\n",
    "    train_text = df.iloc[train_indices]['Text'].values\n",
    "    train_y = df.iloc[train_indices]['Viral'].values\n",
    "    \n",
    "    test_text = df.iloc[test_indices]['Text'].values\n",
    "    test_y = df.iloc[test_indices]['Viral'].values\n",
    "    \n",
    "    pipeline.fit(train_text, train_y)\n",
    "    predictions = pipeline.predict(test_text)\n",
    "\n",
    "    #update totals\n",
    "    confusion += confusion_matrix(test_y, predictions)\n",
    "    score = f1_score(test_y, predictions, pos_label=1)\n",
    "    scores.append(score)\n",
    "t1 = time.time()\n",
    "total = t1-t0\n",
    "print('Time:', total)\n",
    "print('Total tweets classified:', len(df))\n",
    "print('Score:', sum(scores)/len(scores))\n",
    "print('Confusion matrix:')\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Time:', 708.1180000305176)\n",
      "('Total tweets classified:', 59983)\n",
      "('Score:', 0.79366287749578812)\n",
      "Confusion matrix:\n",
      "[[57263    82]\n",
      " [  846  1792]]\n"
     ]
    }
   ],
   "source": [
    "#Make Random Forest clasisifier\n",
    "from sklearn import ensemble\n",
    "# name = \"randomforest\"\n",
    "# classifier = ensemble.RandomForestClassifier(n_estimators=100)\n",
    "# classifier.fit(train_features, train_labels)\n",
    "\n",
    "#Make a pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "import time\n",
    "\n",
    "t0 = time.time()\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer',   NoUrls_CountVectorizer(ngram_range = (1,1), binary =True,\n",
    "                                    min_df=1, stop_words=stop, strip_accents='ascii')),\n",
    "    ('classifier',  ensemble.RandomForestClassifier(n_estimators=10, max_features = 10)) ])\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('binary_unigram', Pipeline([ \n",
    "            ('extract', ColumnExtractor(colName = 'Text')),\n",
    "            ('vectorizer',   NoUrls_CountVectorizer(ngram_range = (1,1), binary =True,\n",
    "                                            min_df=2, stop_words=stop, strip_accents='ascii'))\n",
    "        ])),\n",
    "         ('semicolon_count', Pipeline([\n",
    "            ('extract', PunctuationCount(punctuation = ';')),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "         ('colon_count', Pipeline([\n",
    "            ('extract', PunctuationCount(punctuation = ':')),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('questionmark_count', Pipeline([\n",
    "            ('extract', PunctuationCount(punctuation = '?')),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('exclamation_count', Pipeline([\n",
    "            ('extract', PunctuationCount(punctuation = '!')),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('hyphen_count', Pipeline([\n",
    "            ('extract', PunctuationCount(punctuation = '-')),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('period_count', Pipeline([\n",
    "            ('extract', PunctuationCount(punctuation = '.')),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "         ('quotation_count', Pipeline([\n",
    "            ('extract', PunctuationCount(punctuation = '\"')),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "         ('apostrophe_count', Pipeline([\n",
    "            ('extract', PunctuationCount(punctuation = '\\'')),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "         ('sad_count', Pipeline([\n",
    "            ('extract', SadCount()),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('happy_count', Pipeline([\n",
    "            ('extract', HappyCount()),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('phone_count', Pipeline([\n",
    "            ('extract', PhoneCount()),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('email_count', Pipeline([\n",
    "            ('extract', EmailCount()),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('hashtag_count', Pipeline([\n",
    "            ('extract', HashTagCount()),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('mention_count', Pipeline([\n",
    "            ('extract', MentionCount()),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('link_count', Pipeline([\n",
    "            ('extract', ColumnExtractor(colName = 'LinkCount')),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('hourofday', Pipeline([\n",
    "            ('gethour', HourOfDayTransformer()),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "    ])),\n",
    "    ('classifier',  ensemble.RandomForestClassifier(n_estimators=10, max_features = 10))) \n",
    "])\n",
    "\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "#adapted from http://zacstewart.com/2015/04/28/document-classification-with-scikit-learn.html\n",
    "k_fold = KFold(n=len(df), n_folds=10)\n",
    "scores = []\n",
    "confusion = np.array([[0, 0], [0, 0]])\n",
    "\n",
    "for train_indices, test_indices in k_fold:\n",
    "    train_text = df.iloc[train_indices]['Text'].values\n",
    "    train_y = df.iloc[train_indices]['Viral'].values\n",
    "    \n",
    "    test_text = df.iloc[test_indices]['Text'].values\n",
    "    test_y = df.iloc[test_indices]['Viral'].values\n",
    "    \n",
    "    pipeline.fit(train_text, train_y)\n",
    "    predictions = pipeline.predict(test_text)\n",
    "\n",
    "    #update totals\n",
    "    confusion += confusion_matrix(test_y, predictions)\n",
    "    score = f1_score(test_y, predictions, pos_label=1)\n",
    "    scores.append(score)\n",
    "t1 = time.time()\n",
    "total = t1-t0\n",
    "print('Time:', total)\n",
    "print('Total tweets classified:', len(df))\n",
    "print('Score:', sum(scores)/len(scores))\n",
    "print('Confusion matrix:')\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Time:', 235.39699983596802)\n",
      "('Total tweets classified:', 56131)\n",
      "('Score:', 0.85985592153478607)\n",
      "Confusion matrix:\n",
      "[[27375  2847]\n",
      " [ 4208 21701]]\n"
     ]
    }
   ],
   "source": [
    "#Make naive_bayes\n",
    "from sklearn import naive_bayes\n",
    "# name = \"randomforest\"\n",
    "# classifier = ensemble.RandomForestClassifier(n_estimators=100)\n",
    "# classifier.fit(train_features, train_labels)\n",
    "\n",
    "#Make a pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "import time\n",
    "\n",
    "t0 = time.time()\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer',   NoUrls_CountVectorizer(ngram_range = (1,1), binary =True,\n",
    "                                    min_df=1, stop_words=stop, strip_accents='ascii')),\n",
    "    ('classifier',  naive_bayes.BernoulliNB()) ])\n",
    "\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('binary_unigram', Pipeline([ \n",
    "            ('extract', ColumnExtractor(colName = 'Text')),\n",
    "            ('vectorizer',   NoUrls_CountVectorizer(ngram_range = (1,1), binary =True,\n",
    "                                            min_df=2, stop_words=stop, strip_accents='ascii'))\n",
    "        ])),\n",
    "         ('semicolon_count', Pipeline([\n",
    "            ('extract', PunctuationCount(punctuation = ';')),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "         ('colon_count', Pipeline([\n",
    "            ('extract', PunctuationCount(punctuation = ':')),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('questionmark_count', Pipeline([\n",
    "            ('extract', PunctuationCount(punctuation = '?')),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('exclamation_count', Pipeline([\n",
    "            ('extract', PunctuationCount(punctuation = '!')),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('hyphen_count', Pipeline([\n",
    "            ('extract', PunctuationCount(punctuation = '-')),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('period_count', Pipeline([\n",
    "            ('extract', PunctuationCount(punctuation = '.')),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "         ('quotation_count', Pipeline([\n",
    "            ('extract', PunctuationCount(punctuation = '\"')),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "         ('apostrophe_count', Pipeline([\n",
    "            ('extract', PunctuationCount(punctuation = '\\'')),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "         ('sad_count', Pipeline([\n",
    "            ('extract', SadCount()),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('happy_count', Pipeline([\n",
    "            ('extract', HappyCount()),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('phone_count', Pipeline([\n",
    "            ('extract', PhoneCount()),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('email_count', Pipeline([\n",
    "            ('extract', EmailCount()),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('hashtag_count', Pipeline([\n",
    "            ('extract', HashTagCount()),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('mention_count', Pipeline([\n",
    "            ('extract', MentionCount()),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('link_count', Pipeline([\n",
    "            ('extract', ColumnExtractor(colName = 'LinkCount')),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('hourofday', Pipeline([\n",
    "            ('gethour', HourOfDayTransformer()),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "    ])),\n",
    "    ('classifier',  naive_bayes.BernoulliNB()) \n",
    "])\n",
    "\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "#adapted from http://zacstewart.com/2015/04/28/document-classification-with-scikit-learn.html\n",
    "k_fold = KFold(n=len(df), n_folds=10)\n",
    "scores = []\n",
    "confusion = np.array([[0, 0], [0, 0]])\n",
    "\n",
    "for train_indices, test_indices in k_fold:\n",
    "    #train_text = df.iloc[train_indices]['Text'].values\n",
    "    train_text = df.iloc[train_indices]\n",
    "    train_y = df.iloc[train_indices]['Viral'].values\n",
    "    \n",
    "    #test_text = df.iloc[test_indices]['Text'].values\n",
    "    test_text = df.iloc[test_indices]\n",
    "    test_y = df.iloc[test_indices]['Viral'].values\n",
    "    \n",
    "    pipeline.fit(train_text, train_y)\n",
    "    predictions = pipeline.predict(test_text)\n",
    "\n",
    "    #update totals\n",
    "    confusion += confusion_matrix(test_y, predictions)\n",
    "    score = f1_score(test_y, predictions, pos_label=1)\n",
    "    scores.append(score)\n",
    "t1 = time.time()\n",
    "total = t1-t0\n",
    "print('Time:', total)\n",
    "print('Total tweets classified:', len(df))\n",
    "print('Score:', sum(scores)/len(scores))\n",
    "print('Confusion matrix:')\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('binary_unigram', Pipeline([ \n",
    "            ('extract', ColumnExtractor(colName = 'Text')),\n",
    "            ('vectorizer',   NoUrls_CountVectorizer(ngram_range = (1,1), binary =True,\n",
    "                                            min_df=2, stop_words=stop, strip_accents='ascii'))\n",
    "        ])),\n",
    "         ('semicolon_count', Pipeline([\n",
    "            ('extract', PunctuationCount(punctuation = ';')),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "         ('colon_count', Pipeline([\n",
    "            ('extract', PunctuationCount(punctuation = ':')),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('questionmark_count', Pipeline([\n",
    "            ('extract', PunctuationCount(punctuation = '?')),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('exclamation_count', Pipeline([\n",
    "            ('extract', PunctuationCount(punctuation = '!')),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('hyphen_count', Pipeline([\n",
    "            ('extract', PunctuationCount(punctuation = '-')),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('period_count', Pipeline([\n",
    "            ('extract', PunctuationCount(punctuation = '.')),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "         ('quotation_count', Pipeline([\n",
    "            ('extract', PunctuationCount(punctuation = '\"')),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "         ('apostrophe_count', Pipeline([\n",
    "            ('extract', PunctuationCount(punctuation = '\\'')),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "         ('sad_count', Pipeline([\n",
    "            ('extract', SadCount()),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('happy_count', Pipeline([\n",
    "            ('extract', HappyCount()),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('phone_count', Pipeline([\n",
    "            ('extract', PhoneCount()),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('email_count', Pipeline([\n",
    "            ('extract', EmailCount()),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('hashtag_count', Pipeline([\n",
    "            ('extract', HashTagCount()),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('mention_count', Pipeline([\n",
    "            ('extract', MentionCount()),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('link_count', Pipeline([\n",
    "            ('extract', ColumnExtractor(colName = 'LinkCount')),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "        ('hourofday', Pipeline([\n",
    "            ('gethour', HourOfDayTransformer()),\n",
    "            ('normalize',   NormalizeShape()),\n",
    "        ])),\n",
    "    ]))\n",
    "])\n",
    "X = pipeline.fit(df, df[\"Viral\"])\n",
    "print X.shape\n",
    "print X.columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chi2score = chi2(train_fea, df[\"Viral\"])[0]\n",
    "\n",
    "from pylab import barh,plot,yticks,show,grid,xlabel,figure\n",
    "figure(figsize=(10,20))\n",
    "wscores = zip(vectorizer.get_feature_names(), chi2score)\n",
    "wchi2 = sorted(wscores, key=lambda x:x[1])\n",
    "k=100\n",
    "topchi2 = zip(*wchi2[-k:])\n",
    "x = range(len(topchi2[1]))\n",
    "labels = topchi2[0]\n",
    "barh(x,topchi2[1],align='center',alpha=.2,color='g')\n",
    "plot(topchi2[1],x,'-o',markersize=2,alpha=.8,color='g')\n",
    "yticks(x,labels)\n",
    "xlabel('$\\chi^2$')\n",
    "show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
